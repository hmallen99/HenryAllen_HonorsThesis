\documentclass[../main.tex]{subfiles}

\begin{document}
\subsection{Data Collection}
\subsubsection{Participants}

\subsubsection{Experimental Design}

\subsubsection{Imaging}


\subsection{MEG Analysis}
\subsubsection{Preprocessing}
MEG analysis was performed in Python with the MNE package. Analysis was performed on 204 planar gradiometers (the 102 magnetometers were not used for analysis). Epochs were rejected for a subject if maximum peaks for any gradiometer exceeded a threshold of $4000\times10^{-13}  \frac{T}{m}$. MEG data was then band-pass filtered from 2-40hz, removing any potential low-frequency artifacts and unused high-frequency data. Independent Component Analysis (ICA) for each subject was calculated to remove artifacts caused by EMG and EOG artifacts. ICA performs source separation for statistically independent components of signals. Eighty Independent components were generated for each subject, and independent components resembling EMG and EOG artifacts were visually selected and then excluded from the filtered MEG dataset. An example of these components is seen in figure \ref{ica_exclude}. Here, we see that the left component represents artifacts resulting from eye blinks, while the right component represents muscle movements from the head that we wish to exclude. Trials were epoched at the onset of the the gabor patch stimulus. Trials were performed in batches of 100 trials that were concatenated for each subject. Analysis was performed on these aggregated sessions.

\begin{figure}
    \centering
    \includegraphics{figures/methods/ica_figure.PNG}
    \caption{Example ICA Components to be Excluded}
    \label{ica_exclude}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[scale=0.8]{figures/methods/good_topomap.PNG}
    \caption{Example of a Good ERF for a Subject}
    \label{good_topomap}
\end{figure}

\subsubsection{ERF Analysis}
The averaged evoked responses for each subject were measured to ensure that visual responses
appeared in the occipital area as expected. Time-locked epoch responses were averaged over all
trial blocks for each subject at each electrode and visually analyzed for aberrations. In a good subject, like in figure \ref{good_topomap}, we get evoked responses in left hemisphere occipital and temporal electrodes around 0.2 seconds after the stimulus is shown. The stimulus was only shown in the right visual field, so we see limited activation in the right hemisphere.

\subsection{Source Localization Analysis}
\subsubsection{Preprocessing}

\subsubsection{Source Localiztion Pipeline}


\subsection{Machine Learning Analysis}
\subsubsection{Machine Learning Methods}
Analysis was performed in Python using the Keras library from TensorFlow, decoding libraries from MNE-Python, and the sklearn library. Data was split into training sets of 500 trials for each subject with 100 trials of test trials. Trials were time-binned from 0-0.4 seconds at 0.025 second intervals to account for the 40hz MEG data. The data is split into classes based on angle. For example, if we wanted 4 classes, class 0 corresponds to gabors oriented from 0-45 degrees, class 1 from 45-90, class 2 from 90-135, class 3 from 135-180. An issue is immediately apparent from these class divisions. A gabor oriented at 179 degrees is marked as class 3 despite being very similar to a gabor marked in class 1, at 1 degree for example. To account for this, we can use a few tactics: (1) using many classes to minimize the inaccuracy of any one individual class, (2) split the first class between 180 degrees and 0 degrees, or (3) develop a cost function that takes the cosine encoding into account. Models were trained for each subject to account for individual Model accuracy was calculated with 5-fold cross-validation and evaluation accuracy on the test dataset. Model accuracy was then compared with the results of a permutation test, in which we shuffled around the training and test labels of the dataset to calculate a baseline accuracy for a naive model. This test essentially compares our models to a chance accuracy.

We used source localization to clarify the readings we received from the electrodes. To validate that the source localization data actually improves our accuracy, or at least gives us equivalent performance, we ran our experiments on inputs from the MEG electrodes (the naive model) and the MRI vertices (the source localization model). There were many more MRI vertices than MEG electrodes, so the weight matrices for the source localization models were much larger than the electrode models. This shouldn't impact the results, but it does mean that we need to tune our hyperparameters differently for electrode and MRI models.
%TODO: explain what hyperparameter tuning is

\subsubsection{Sliding Logistic Regression Model}
The machine learning model used for our control model was a sliding logistic regression model, which calculates a separate accuracy for each timestep of our MEG input signal. For 16 timesteps, we calculate 16 different logistic regression weights, and update these weights using a gradient descent procedure while training. The model was regularized with an elastic net regularization term, which acts as a combination of L1 and L2 regression. This regularization term helps enforce sparsity within our model and prevents weights that are too large, combining the traditional benefits of L1 and L2 regularization. The model also used a "select K best" routine that selects the $K$ best features from the model, based on their contributions to the accuracy. We then had to tune the hyperparameters for these additions to our model, namely the size of $K$, the ratio between L1 and L2 loss for elasticnet, and the regularization parameter $C$. The hyperparameters were tuned to get the best cross-validation accuracy for 500 trials.
%TODO: Final HyperParameters?

%TODO: How does Logistic regression work

The logistic regression model was used for its simplicity and interpretability. The model is relatively easy to implement, especially with the machine learning packages that are already built into scikit-learn and mne-python. The model has only one layer, making it relatively fast to run the model and tune the model parameters. This also means that we can interpret the weights quite easily, as there is a one-to-one correspondence between a particular weight and a reading from an electrode or MRI vertex. This tells us that if a particular weight has a high value, the corresponding electrode/vertex is weighted highly in the model.

\subsubsection{Sliding Neural Network Model}
We also tested our control model with a sliding neural network (SNN). Similar to the sliding logistic regression model, the SNN calculates separate predictions and model weights for each timestep in the input, but we instead train a neural network instead of a logistic regression model. The SNN is much more complex than the logistic regression model, as it has multiple layers, activation functions between each layer, and multiple hyperparameters to tune. Particularly, we have to choose the layer sizes and batch sizes.

%TODO: Brief overview on Neural Networks

\subsubsection{Recurrent Neural Network Model}

\subsubsection{Serial Dependence Model}


\end{document}